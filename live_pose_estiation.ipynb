{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b19a09",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For this final project, I will try to make an app that will detect pose estimation from photos. The user can upload a photo; they will get a 2d skeleton pose estimation of the body. \n",
    "\n",
    "\n",
    "For the prototype, I tried to train a model which detect face pose and also tried to work with a pre-trained model which detect yoga pose. I also manage to creat a real time pose estimation model. (_live_pose_estimation_).\n",
    "\n",
    "\n",
    "First I trained a model that can detect the face pose (_face_pose_detection.ipynb_ ). For this model, I have used a cv2, matplotlib, tensorflow, keras, numPy, pandas, and a face key point detection dataset from Kaggle. By using panda, the model is reading the dataset, numpy is converting the image to an array and then I made it between 0 – 255, and added the key point on the face. To train the model, I have used layers of activation relu, 40 epochs, and 16 batch sizes. After I trained the model I used the test data to predict and tried to add the key point on the image for the first 30 photos on the dataset. \n",
    "\n",
    "\n",
    "I am really interested in full-body movement. I am aiming to use this project to use it for the yoga movement. So, after I trained the face detector model, I have also tried to find another yoga dataset from kaggle and used a pre-trained model (_yoga_pose_estimation.ipynb_) to do an experiment to detect yoga body posture estimation from photos. This dataset, using the most common 5 different poses of yoga which are the downward dog pose, goddess pose, tree pose, plank pose, and warrior pose. My target was if a user uploads a photo the app will detect the pose and it will make a 2d skeleton pose estimation of the photo.\n",
    "\n",
    "\n",
    "For this pre-trained model, the libraries are cv2, mediapipe, ,numpy, matplotlib, and glob. By using cv2.imread it read the image and numpy changes the image to an integer array, find the detection, and joins line on that to detect the pose. For this model, I am using the first 30 photos from the dataset to test. \n",
    "\n",
    "\n",
    "### Test:\n",
    "\n",
    "To test **live_pose_estimation**, run the code and it will use the webcamera of the laptop and show the live pose estimation. to quit the camera use ** q ** on the keyboard.\n",
    "\n",
    "To test **face_pose_detection.ipynb**, I have used 30 as range but you can change the range from cell 9 and that will change the image and the number of images as the output of face pose estimation from the dataset.\n",
    "\n",
    "\n",
    "To test **yoga_pose_estimation.ipynb**, you can change value of \"i\" on cell 5,6,7 or you can add another cell at the end with “pyplot.imshow(images[i])”. The “i” should be between the range. For this model, I am using 30 as my range but you can change the range from cell 4 and use the value of \"i\" between the range. \n",
    "\n",
    "You can also upload photo in the folder yoga_images (input > yoga_images) and find the pose estimation of the body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf33aa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\diana\\anaconda3\\lib\\site-packages (0.8.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\diana\\anaconda3\\lib\\site-packages (4.5.1.48)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\diana\\anaconda3\\lib\\site-packages (from mediapipe) (0.6)\n",
      "Requirement already satisfied: absl-py in c:\\users\\diana\\anaconda3\\lib\\site-packages (from mediapipe) (0.15.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\diana\\anaconda3\\lib\\site-packages (from mediapipe) (20.3.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\diana\\anaconda3\\lib\\site-packages (from mediapipe) (0.36.2)\n",
      "Requirement already satisfied: numpy==1.19.3 in c:\\users\\diana\\anaconda3\\lib\\site-packages (from mediapipe) (1.19.3)\n",
      "Requirement already satisfied: six in c:\\users\\diana\\anaconda3\\lib\\site-packages (from mediapipe) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in c:\\users\\diana\\anaconda3\\lib\\site-packages (from mediapipe) (3.18.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfca7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5483292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the webcamera\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret,frame = cap.read()\n",
    "    \n",
    "        #recolour image to rgb:\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        #make detection:\n",
    "        results = pose.process(image)\n",
    "\n",
    "        #To BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        #render detection:\n",
    "        mp_drawing.draw_landmarks(image,results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70fcb2d",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "To test run the code and it will use the webcamera of the laptop and show the live pose estimation. to quit the camera use **q** on the keyboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6236065",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "\n",
    "face_pose :\n",
    "https://www.kaggle.com/lqdisme/facial-keypoint-detection-cnn-augmentation\n",
    "\n",
    "dataset for yoga_pose:\n",
    "https://www.kaggle.com/niharika41298/yoga-poses-dataset\n",
    "\n",
    "https://youtu.be/06TE_U21FK4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
